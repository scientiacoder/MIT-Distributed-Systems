# MIT 6.824ï¼š Distributed Systems
**æœ¬Repoè®°å½•äº†MIT 6.824è¯¾ç¨‹ç›¸å…³çš„ç¬”è®°ä»¥åŠæ”¶é›†äº†RAFTç›¸å…³å¥½çš„æ–‡ç« å’Œè§†é¢‘**
  
**è¯¾ç¨‹**Youtube é“¾æ¥: https://www.youtube.com/watch?v=cQP8WApzIQQ&list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB&index=1&ab_channel=MIT6.824%3ADistributedSystems  
  
**è¯¾ç¨‹**Labçš„åœ°å€: https://pdos.csail.mit.edu/6.824/  
  
Raft**è¯ç”Ÿçš„è®ºæ–‡**(ä¸ºä»€ä¹ˆè¦Raft, Paxosä¸ºä»€ä¹ˆä¸å¥½)(è®ºæ–‡é‡Œçš„Figure 2çœŸçš„å¾ˆé‡è¦): https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf
  
**Raftä½œè€…äº²è‡ªè®²è§£**Raft: https://www.youtube.com/watch?v=vYp4LYbnnW8&t=94s&ab_channel=DiegoOngaro
  
ä¸€ä¸ªä¸é”™çš„Raftæ–‡ç« (é€‚åˆæ–°æ‰‹): https://kasunindrasiri.medium.com/understanding-raft-distributed-consensus-242ec1d2f521
  
Raft**åŠ¨ç”»åœ¨çº¿æ¼”ç¤º**(æ˜“äºç†è§£): http://thesecretlivesofdata.com/raft/
  
æŸä½å›½å†…å¤§ä½¬çš„åšå®¢(é‡Œé¢æœ‰labå®ç°): https://yuerblog.cc/2020/08/13/mit-6-824-distributed-systems-%e5%ae%9e%e7%8e%b0raft-lab2a/
  
æ ¹æ®è¯¾ç¨‹è§„å®šï¼Œæœ¬Repoå°†ä¸ä¼šå…¬å¼€MIT-6.824 Labç›¸å…³çš„ä»£ç å’Œè§£å†³æ–¹æ¡ˆï¼Œä»¥é¼“åŠ±æ›´å¤šçš„ä¼˜è´¨è¯¾ç¨‹å¼€æºåˆ†äº«  
According to the course rules and code of conduct, the Lab code and solution associated with the MIT-6.284 course will not be open sourced in order
to encourge sharing of high quality courses like this.
  
## Lecture 1 Introduction
 1. èƒ½ç”¨ä¸€å°æœºå™¨è§£å†³çš„é—®é¢˜ï¼Œå°±ä¸è¦ç”¨åˆ†å¸ƒå¼
 2. Scalability - 2x Computers -> 2x Throughput
 3. Fault Tolerance:
     - Availability
     - Recoverability:
        - NV(non-volatile storage) Storageï¼š æ–­ç”µåèƒ½æ¢å¤ï¼Œä½†æ˜¯ä»£ä»·æ˜‚è´µï¼Œexpensive to update, åº”è¯¥é¿å…ä½¿ç”¨
        - Replication: identical servers æ¨è
     - Strong Consistency: éå¸¸expensive, ä¸æ¨è 10msç½‘ç»œå»¶è¿Ÿcommunication cost, èƒ½ä¸¢å¤±ç™¾ä¸‡æ¡æŒ‡ä»¤
     - **Weak** Consistency: æ¨è
 4. MapReduce:
    - Reduceè¿™ä¸€å—è¦ç”¨åˆ°ç½‘ç»œé€šä¿¡ï¼Œç½‘ç»œé€šä¿¡è¦æ¶ˆè€—å¸¦å®½ï¼Œå¦‚æœæœ‰10TBæ–‡ä»¶è¦MapReduceï¼Œåˆ™Reduceé˜¶æ®µè¦åœ¨ç½‘ç»œä¸­å‘é€10TBï¼Œæœ€åäº§å‡º10TBï¼Œç½‘ç»œæ¶ˆè€—éå¸¸å¤§
 5. åˆ†å¸ƒå¼ç³»ç»Ÿè¦ç”¨åˆ°å¾ˆå¤šçš„ç½‘ç»œé€šä¿¡ï¼Œç½‘ç»œè¿™ä¸€å—å¾ˆé‡è¦

## Lecture 2&3 RPC and Threads & GFS
 1. MITæ¨èä¹¦ç±**Effective Go**
 2. ```go run -race xxx.go```å¯ä»¥ç”¨æ¥è¿›è¡Œç«äº‰æ£€æŸ¥
 ```go
 ch := make(chan []string)
 for urls := range ch{
    update ch // è¿™ä¸ªforå¯èƒ½æ°¸è¿œéƒ½ä¸ä¼šåœæ­¢
 }
 ```
 ![bad-repl](./imgs/bad-repl.png)  
 S1å’ŒS2å¤„ç†C1ã€C2è¯·æ±‚çš„(æ—¶é—´)é¡ºåºä¸åŒï¼Œå‡ºç°ä¸åŒçš„ç»“æœï¼Œæ²¡æœ‰consistencyï¼Œæ‰€ä»¥æ˜¯bad design  
 1. GFSæ˜¯åœ¨ä¸€ä¸ªData centeré‡Œï¼Œè€Œä¸æ˜¯å…¨çƒéƒ½æœ‰around the world
 2. GFSæ˜¯Big sequential access, å°±æ˜¯GBã€TBå¤§æ–‡ä»¶ï¼Œè€Œä¸æ˜¯randomçš„  
 3. GFSæ˜¯åŸºäºMaster Workerçš„ï¼ŒMaster DATAè§ä¸‹å›¾
 4. version è¡¨ç¤ºæœ€æ–°çš„ç‰ˆæœ¬ writeçš„æ—¶å€™è¦æ¯”è¾ƒï¼Œç„¶ååœ¨æœ€æ–°çš„ç‰ˆæœ¬append
 
 ![master-data](./imgs/master-data.png)
   
 ä»¥ä¸‹æ˜¯GFS writeæ—¶çš„æ“ä½œï¼Œå·¦è¾¹INCREMENT V#æ˜¯æŒ‡Master increment, å³è¾¹å¦‚æœprimary returns "no" to client, åˆ™clientä¼šç»§ç»­å‘èµ·è¿™ä¸ªå†™è¯·æ±‚
 ç›´åˆ°æˆåŠŸä¸ºæ­¢ï¼ŒGoogleå¥½åƒå¹¶æ²¡æœ‰æä¸æˆåŠŸçš„æƒ…å†µ  
 ![gfs-write](./imgs/gfs-write.png)  
  - **GFSæ²¡æœ‰Strong Consistency, æ‰€ä»¥å­˜çš„ä¸œè¥¿å¯èƒ½ä¸¢å¤±ã€é‡å¤ï¼**
  - å¦‚æœPimaryæŒ‚äº†ï¼Œåœ¨Secondaryé€‰ä¸¾ä¸ºæ–°çš„Primaryåï¼Œè¦è¿›è¡ŒåŒæ­¥Sync, å› ä¸ºå¯èƒ½æœ‰Last set operationæœ‰çš„åšäº†æœ‰çš„æ²¡åš
  - GFSä½¿ç”¨å•ä¸€èŠ‚ç‚¹ä½œä¸ºMasterï¼Œå…¶å®æ˜¯æœ‰é—®é¢˜çš„ï¼Œå‡ºç°Out of Memory RAMï¼Œæ‰€æœ‰ä¸œè¥¿éƒ½åœ¨å†…å­˜é‡Œï¼ŒåŠ å†…å­˜ä¸å¤Ÿ
   
 ## Lecture 4 Primary-Backup Replication ä¸»ä»å¤åˆ¶
  1. **State transfer**: Primary sends state to the secondary(Sync) e.g Send memory to the backup.
      - sends **memory**
  2. **Replicated State Mache**: Assuming state transfer is deterministic unless there are external factors. Thus, Primary do not send state, instead , it sends
  external factors to the secondary (backup).
      - sends **operations from client**
  3. What state? ä¸»ä»å¤åˆ¶è€ƒè™‘çš„é—®é¢˜
      - Primary/Backup Sync ä¸»ä»åŒæ­¥
      - Cut-over ä¸»ä»è½¬æ¢ï¼Œæ¯”å¦‚ä¸»æœºæŒ‚äº†éœ€è¦æå‡ä¸€ä¸ªæ–°çš„ä¸»æœº
      - Anomalies å¤–ç•Œä¸åº”è¯¥æ„ŸçŸ¥åˆ°ä¸»æœºæŒ‚äº†
      - New replicas å¤‡ä»½ä»æŒ‚äº†ï¼Œä¹Ÿéœ€è¦ä¸€ä¸ªæ–°çš„å¤‡ä»½
  4. ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯Primaryæ”¶åˆ°ä¸€æ¡æŒ‡ä»¤(packet?)ï¼Œç„¶åå‘ç»™Backupä¹Ÿæ‰§è¡Œè¿™æ¡æŒ‡ä»¤(packet)ï¼Œä½†æ˜¯æœ‰ä¸€äº›æŒ‡ä»¤æ˜¯weird instructionsæ¯”å¦‚è·å–å½“å‰æ—¶é—´ï¼Œè·å¾—è¿›ç¨‹IDï¼Œéšæœºæ•°ä¹‹ç±»çš„ï¼Œ
  åœ¨ä¸»å’Œä»ä¼šäº§ç”Ÿä¸åŒçš„ç»“æœï¼Œè¿™ç§æƒ…å†µä¸‹ä»éœ€è¦ç­‰ä¸»æ‰§è¡Œå®Œåå°†ç»“æœå‘Šè¯‰å®ƒ
  5. **Output Rule**: Primary can not generate output(ACK) to the client until all the backups generate output to the primary.
  6. ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œclientå’ŒPrimaryä»¥åŠSecondaryè¿æ¥ä½¿ç”¨**TCPåè®®**ï¼ŒPrimaryæŠŠTCPåŒ…è½¬ç»™Secondaryï¼Œå›å¤ackæ—¶ä¼šæŠŠ**tcp Sequence number**å¸¦ä¸Šï¼Œæ‰€ä»¥
  å¦‚æœclientæ”¶åˆ°ä¸¤ä»½ack(primary and secondaryï¼‰é€šè¿‡æŸ¥çœ‹Seqï¼Œä¼šæŠŠç¬¬äºŒä¸ªSecondaryå‘çš„ç»™dropæ‰
  7. **Split Brain**(è„‘è£‚)çš„è§£å†³ï¼šè®©ç¬¬ä¸‰æ–¹authorityæ¥ç¡®å®šè°æ˜¯Primary, æœ‰ä¸€ä¸ªTEST-AND-SET Serverï¼Œè®©å·¦è„‘å’Œå³è„‘åŒæ—¶å‘é€test-and-setå‘½ä»¤ç»™è¿™ä¸ªServerï¼Œ
  Serverå…ˆæ”¶åˆ°è°çš„å‘½ä»¤å°±è®©è°setä¸ºçœŸæ­£çš„Primary.
   
 ## Lecture 5 Concurrency in Go
```go
// åœ¨forä¸­ä½¿ç”¨goroutineçš„æ—¶å€™ï¼Œå¦‚æœç”¨åˆ°iï¼Œè¦ç”¨go func(x int)çš„å½¢å¼å°†iä¼ è¿›å»
// å› ä¸ºforä¼šæŠŠiçš„å€¼æ”¹æ‰ï¼Œå› æ­¤å½“goroutineè¿è¡Œåˆ°è¿™ä¸€è¡ŒsendRPC(x or i)è¿™ä¸€è¡Œæ—¶
// iä¼šå˜æˆforæ›´æ”¹åçš„å€¼``
func main() {
	var wg sync.WaitGroup
	for i := 0; i < 5; i++ {
		wg.Add(1)
		go func(x int) {
			sendRPC(x)
			wg.Done()
		}(i)
	}
	wg.Wait()
}

func sendRPC(i int) {
	println(i)
}

```
### æ¥ä¸‹æ¥æ˜¯ç”¨goåšä¸€äº›å‘¨æœŸæ€§çš„äº‹æƒ…, periodic
```go
// periodicæ¨¡å‹
var done bool
var mu sync.Mutex

func main() {
	time.Sleep(1 * time.Second)
	println("started")
	go periodic()
	time.Sleep(5 * time.Second)
	mu.Lock()
	done = true
	mu.Unlock()
	println("cancelled")
	time.Sleep(3 * time.Second) // observe no output
}

func periodic() {
	for {
		println("tick")
		time.Sleep(1 * time.Second)
		mu.Lock()
		if done {
			// è¿™æ˜¯ä¸ªbug, å› ä¸ºreturnå‰æ²¡æœ‰Unlock()!åŠ ä¸Š
			mu.Unlock()
			return
		}
		mu.Unlock()
	}
}

```
### deferå°çŸ¥è¯†
```go
func main() {
	println("started")
	defer println("1")
	defer println("2")
	defer println("3")
	defer println("4")
	defer println("5")
}
// è¿™æ®µä»£ç ä¼šæ‰“å° deferå°çŸ¥è¯†ï¼šæŒ‰deferçš„é¡ºåºç†è§£ä¸ºå…¥æ ˆï¼Œæœ€åå‡ºæ ˆ
started
5
4
3
2
1
```
### Busy waitingä»¥åŠè§£å†³(sleepç­‰å¾…æˆ–è€…ä½¿ç”¨condition)
```go
// é¦–å…ˆæ¥çœ‹è¿™ä¸€æ®µä»£ç , è¿™æ®µä»£ç ä¸ºä»€ä¹ˆä¸è¡Œï¼Œå°±æ˜¯å› ä¸ºå®ƒä¸€ç›´åœ¨å¿™ç­‰å¾…(busy waiting)ï¼Œforä¸€ç›´å°è¯•è·å–é”
// è¿™æ ·ä¼šå¸¦æ¥å¤§é‡çš„cpuæ¶ˆè€—ï¼Œæ‰€ä»¥ä¸€å®šè¦é¿å…å¿™ç­‰busy waiting
for {
    mu.Lock()
    if count >=  || finished == 10{
    	break
    }
    mu.Unlock()
}
// do something
mu.Unlock()
```
#### ç¬¬ä¸€ç§è§£å†³æ–¹æ¡ˆï¼šåŠ å…¥time.Sleep()ç­‰å¾…
```go
// è¿™ç§æ–¹æ³•ä¼šæœ‰magic numberï¼Œä½†æ˜¯å¯ä»¥è§£å†³å¿™ç­‰çš„é—®é¢˜
for {
    mu.Lock()
    if count >=  || finished == 10{
    	break
    }
    mu.Unlock()
    time.Sleep(50 * time.Millisecond)
}
```
#### ç¬¬äºŒç§è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨Condition(æ¨è)
```go
// Condition broadcast waitç±»ä¼¼äºsignalå’Œwait, åŒºåˆ«æ˜¯signalç”¨äºå”¤é†’ä¸€ä¸ª
// cond.Wait()çš„æ—¶å€™ä¼šè®²è¿™ä¸ªåŠ å…¥åˆ°ä¸€ä¸ªwait listé‡Œç„¶åç­‰å¾…broadcast
// condè¦ç»‘å®šä¸€ä¸ªmutex
cond := sync.NewCond(&mu)
for i := 0 ...{
    go func(){
    	vote := requestVote()
	mu.Lock()
	defer mu.Unlock()
	if vote{
	    count++
	}
	finished++
	cond.Broadcast() // broadcastä¸€å®šè¦åœ¨Unlock()æ“ä½œä¹‹å‰
    }()
}
mu.Lock()
for count < 5 && finished != 10{ // è¿™é‡Œè¦æ˜¯falseçš„åˆ¤æ–­
    cond.Wait()
}
// do something
mu.Unlock()
```
**Conditionæ¨¡ç‰ˆ**ï¼š
```go
mu.Lock()
// do something that might affect the condition
cond.Broadcast()
mu.Unlock()

----

mu.Lock()
while condition == false{
    cond.Wait()
}
// new condition is true, and we have the lock
// do something
mu.Unlock()
```
### go channel will block until other goroutine receives
```go
// è¿™æ®µä»£ç  fatal error: all goroutines are asleep - deadlock!
// c <- trueè¿™è¡Œä»£ç ä¼šä¸€ç›´block å› ä¸ºæ²¡æœ‰receiver
func main() {
	c := make(chan bool)
	c <- true // blocks until other goroutine receives
	<-c
}

```
 1. you should avoid buffered channels use ```make(chan bool)``` instead of ```make(chan bool, 5)```
 2. waitgroup.Add()ä¸€èˆ¬æ˜¯åœ¨go func(){}()ä¹‹å‰ï¼Œè¿™æ ·ä¿è¯åœ¨wg.Wait()ä¹‹å‰
  
## Raftåè®®
MapReudce, GFS, and TEST-AND-SET Serveréƒ½æœ‰ä¸€ä¸ªå…±åŒç‚¹ï¼Œå°±æ˜¯éƒ½æ˜¯åªæœ‰ä¸€ä¸ªMasterèŠ‚ç‚¹æ¥å­˜å‚¨ä¸€äº›å…ƒæ•°æ®ï¼Œè¿™æ ·å°±ä¼šæœ‰**å•ç‚¹æ•…éšœ(Single point of failure)**çš„é—®é¢˜ï¼Œ
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°±è¦å¼•å…¥å¤šå°æœºå™¨æ¥æ•…éšœå®¹é”™
  
ä¸€ä¸ªä¸é”™çš„Raftæ–‡ç« : https://kasunindrasiri.medium.com/understanding-raft-distributed-consensus-242ec1d2f521
  
RaftåŠ¨ç”»åœ¨çº¿æ¼”ç¤º: http://thesecretlivesofdata.com/raft/
  
### Majority Vote å¤§å¤šæ•°é€‰ä¸¾
 1. ç¬¬ä¸€æ­¥æ˜¯è¦æœ‰å¥‡æ•°(odd)å°æœºå™¨ï¼Œè€Œä¸æ˜¯å¶æ•°(even)å°æœºå™¨
 2. ç”¨æˆ·(clients)ä¸çŸ¥é“å®ƒäº¤æµçš„æ˜¯Masterè¿˜æ˜¯Replicaï¼Œåœ¨å¤–ç•Œçœ‹æ¥å¥½åƒåªæœ‰ä¸€å°æœºå™¨
 3. **Paxos**æ²¡æœ‰Leader
 4. æœ‰Leaderçš„å¥½å¤„æ˜¯æ›´åŠ çš„efficient, å¦‚æœæ²¡æœ‰Leaderï¼Œå¯¹äºæ“ä½œé¦–å…ˆå¯èƒ½è¦é€‰å‡ºtemp leaderï¼Œç„¶åè¾¾æˆä¸€è‡´ï¼Œè¿™æ ·ä¸å¤Ÿefficient
 5. each term will have at most 1 leader(could be 0 or 1)
 6. **å¦‚ä½•é€‰Leader?** éœ€è¦Leaderå¾—åˆ°å¤§å¤šæ•°çš„yes voteåœ¨é€‰ä¸¾æœŸé—´ï¼Œä½†æ˜¯åœ¨é€‰ä¸¾æœŸé—´ï¼Œæ¯å°serveræœ€å¤šåªèƒ½æŠ•ä¸€å¼ yes vote, ä¿è¯äº†
 æ¯ä¸ªtermæœ€å¤šåªèƒ½æœ‰ä¸€ä¸ªleader
 7. é€‰ä¸¾çš„æ—¶å€™æœ‰ä¸€ä¸ªtimer, å¦‚æœtimeræ—¶é—´åˆ°äº†ï¼Œä¼šå¼€å¯æ–°ä¸€è½®termçš„é€‰ä¸¾, term++
  
### åŸºäºRaftåè®®çš„KVæ•°æ®åº“å¯ä»¥çœ‹ä½œä¸‹å›¾
![raft-1](./imgs/raft1.jpeg)
  
å‡è®¾æœ‰ä¸‰å°æœåŠ¡å™¨S1,S2,S3, æ¯å°æœåŠ¡å™¨æœ‰åº”ç”¨å±‚(å­˜å‚¨kvæ•°æ®åº“), Raftåè®®å±‚(å­˜å‚¨æ“ä½œlog), æœ‰ä¸¤ä¸ªå®¢æˆ·ç«¯clientå¯¹å…¶è®¿é—®ï¼Œclientå¯ä»¥
æœ‰putå’Œgetæ“ä½œï¼Œé¦–å…ˆclientå¯¹leaderçš„åº”ç”¨å±‚å‘å‡ºæ¯”å¦‚è¯´putè¯·æ±‚ï¼Œæ­¤æ—¶leaderçš„åº”ç”¨å±‚ä¼šé€šçŸ¥Raftå±‚ï¼ŒåŠ å…¥åˆ°æ“ä½œlogé‡Œï¼Œä¸æ­¤åŒæ—¶ï¼ŒRaftå±‚ä¼šå‘å…¶å®ƒæ‰€æœ‰çš„ReplicaèŠ‚ç‚¹è¿›è¡Œé€šä¿¡ï¼Œå‘ŠçŸ¥æ­¤æ“ä½œï¼Œå…¶å®ƒèŠ‚ç‚¹çš„Raftå±‚ä¹Ÿä¼šé€šçŸ¥åº”ç”¨å±‚ï¼Œ**å¹¶ä¸”å¯¹Leaderçš„Raftå±‚è¿›è¡Œå›åº”**ï¼Œæ­¤æ—¶Leaderçš„Raftå±‚ä¼šé€šçŸ¥åº”ç”¨å±‚ï¼Œç„¶ååº”ç”¨å±‚å¯¹clientè¿›è¡Œå›åº”
  
![raft-2](./imgs/raft2.jpeg)
  
ä½†äº‹å®ä¸Šåªè¦Leaderæ”¶åˆ°å¤§å¤šæ•°æ¥è‡ªReplicaçš„å›åº”(>n/2,åŒ…æ‹¬leaderæœ¬èº«)ï¼Œç„¶åleaderå°±ä¼šæ‰§è¡Œè¯¥å®¢æˆ·è¯·æ±‚çš„æ“ä½œï¼Œå¹¶ä¸”ä¼šå¯¹clientè¿›è¡Œå›å¤ï¼Œå‚ç…§ä¸Šå›¾
  
![raft-3](./imgs/raft3.jpeg)
  
ä½†æ˜¯æ­¤æ—¶Replicaå¹¶æœªæ‰§è¡Œå…·ä½“æ“ä½œï¼Œåªæ˜¯å°†å…¶å†™å…¥raft logä¸­ï¼Œåœ¨leaderå¯¹clientå›å¤çš„æ—¶å€™ï¼Œä¹Ÿä¼šå‘é€é€šçŸ¥ç»™replica, åœ¨è¿™æ—¶replicaæ‰çœŸæ­£æ‰§è¡Œå®¢æˆ·å‘½ä»¤
  
### ä¸ºä»€ä¹ˆreplicaä¹‹åæ‰æ‰§è¡Œå‘½ä»¤ï¼Œè€Œä¸æ˜¯ç«‹å³æ‰§è¡Œï¼Ÿ
å› ä¸ºç”¨æˆ·clientå…¶å®å¹¶ä¸å…³å¿ƒreplicaæ˜¯å¦æ‰§è¡Œï¼Œåªæ˜¯å¦åœ¨leaderå¤„æˆåŠŸå†™å…¥, æ‰€ä»¥å¯¹replciaæ¥è¯´å¯ä»¥ä¹‹åæ‰§è¡Œ
  
### ä¸ºä»€ä¹ˆè¿™ä¹ˆå¼ºè°ƒä½¿ç”¨log?
å¯¹replicaæ¥è¯´ä¸å…‰å…³å¿ƒç”¨æˆ·çš„å‘½ä»¤æ˜¯ä»€ä¹ˆï¼Œè¿™äº›å‘½ä»¤çš„**é¡ºåº**åŒæ ·é‡è¦, å¹¶ä¸”æ–¹ä¾¿reboot
  
**åœ¨ä¸€å®šçš„æ—¶é—´åï¼Œä¸åŒæœºå™¨ä¸Šçš„logå¯èƒ½ä¼šdiverge, ä½†æ˜¯Raftä¼šæœ‰æœºåˆ¶å¼ºåˆ¶ä½¿å¾—logå˜æˆç›¸åŒçš„**
  
## Raft Log System
![raft-log1](./imgs/raft-log1.jpeg)
  
å‡è®¾æœ‰ä¸‰å°æœºå™¨ï¼Œæ¯ä¸€å°æœºå™¨çš„Raft log indexå­˜æ”¾äº†é‚£ä¸ªæ—¶æœŸçš„term number, å½“å‡ºç°ä¸ä¸€è‡´çš„æ—¶å€™ï¼Œæ¯”å¦‚åœ¨index 12, S2çš„term
è®°å½•çš„æ˜¯4, S3è®°å½•çš„æ˜¯5, åœ¨index 13çš„æ—¶å€™ï¼ŒS3è¢«é€‰ä¸ºäº†term 6çš„leader, æ­¤æ—¶S3å‘S2å’ŒS1å‘é€Append Entryçš„æ—¶å€™ï¼Œä¼šå‘é€
prevLogIndexå’ŒprevLogTermï¼ŒS2å‘ç°ä¸å…¶ä¸ä¸€æ ·ï¼Œäºæ˜¯ä¼šæ‹’ç»è¿™ä¸ªAppend Entry
  
![raft-log2](./imgs/raft-log2.jpeg)
  
è¿™æ—¶Raft Leaderä¼šæœ‰ä¸€ä¸ªå›æº¯æœºåˆ¶ï¼ŒnextIndex[S2]=12, ä¸€ç›´å¾€å‰å›æº¯ï¼Œå‘ç°åœ¨index 11çš„æ—¶å€™S2å’ŒS3æ˜¯ä¸€æ ·çš„ï¼Œäºæ˜¯S2å°†å…¶index 12
å’Œ13è®¾ç½®ä¸ºä¸term 6 Leader S3ä¸€è‡´çš„log, åŒç†å¯¹S1åˆ™ç»§ç»­å›æº¯ï¼Œå‘ç°åœ¨index 10çš„æ—¶å€™ä¸€è‡´
  
### Persistent
 - Log
 - CurrentTerm
 - VoteFor
  
## Linearizability å¯çº¿æ€§åŒ–(alias for strong consistency)
åˆ†å¸ƒå¼æœºå™¨æ˜¯å¦å¯ä»¥è¡¨ç°å¾—åƒä¸€å°**å•æœº**åœ¨è¿è¡Œä¸€æ ·ï¼Œæˆä¸ºå¯çº¿æ€§åŒ–(Linearizability)
  
**Linearizability**: **Execution history**(sequence of requests by many clients) is Linearizable **IF** there 
exists some **total order**(one by one..) of operation history that matches real time for non-concurrent request and each 
read sees the value of the most recent write in the order(å°±æ˜¯æ‰¾ä¸€ä¸ªåºåˆ—ï¼Œè¿™ä¸ªåºåˆ—å°±åƒåœ¨å•æœºä¸Šè¿è¡Œä¸€æ ·ï¼Œæ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶)**constraints**:
 - If one operation **finish before** another started, then the one finish first has to come first in the history
 - If some read sees a **particluar** written value, then the read must comes after the write in the order
  
ä¸¤ä¸ªæ“ä½œï¼Œå¦‚æœä¸€ä¸ªæ“ä½œç»“æŸåœ¨å¦ä¸€ä¸ªæ“ä½œå¼€å§‹ä¹‹å‰ï¼Œé‚£ä¹ˆå…ˆç»“æŸçš„åœ¨å‰é¢  
å¦‚æœä¸€ä¸ªreadè¯»åˆ°äº†ä¸€ä¸ªå€¼, é‚£ä¹ˆè¿™ä¸ªreadæ“ä½œä¸€å®šåœ¨å†™writeè¿™ä¸ªå€¼çš„æ“ä½œä¹‹å
  
ä¸¾ä¸ªæ —å­ğŸŒ°
```
Example 1

|---Wx1---|   |---Wx2---|
      |---Rx2----|
      	|--Rx1--|
è¯´æ˜Rx1åœ¨Wx1åé¢, Rx2åœ¨Wx2åé¢, Wx2åœ¨Wx1åé¢
total order: Wx1 Rx1 Wx2 Rx2
```
```
Example 2:

|---Wx1---|   |---Wx2---|
      |---Rx2----|
      		    |--Rx1--|
Wx2åœ¨Rx2å‰é¢, Rx2åœ¨Rx1å‰é¢(finish first), Rx1åœ¨Wx2å‰é¢(å› ä¸ºè¯»åˆ°Wx1å†™çš„)
å‡ºç°å¾ªç¯: Wx2->Rx2->Rx1->Wx2, æ‰€ä»¥ä¸å­˜åœ¨total order
```
```
Example 3:

|---Wx0---|   |---Wx1---|
		 |---Wx2---|
Client 1:   |---Rx2--| |--Rx1--|
Client 2:   |---Rx1--| |--Rx2--|

è¿™ç§æƒ…å†µä¸‹ä¹Ÿä¸å­˜åœ¨total order, å› ä¸ºå¦‚æœå­˜åœ¨, Execution logåº”è¯¥æ˜¯å”¯ä¸€ç¡®å®šçš„é¡ºåº, Client1å’ŒClient2çœ‹åˆ°
åº”è¯¥æ˜¯ä¸€æ ·çš„ æ‰€ä»¥ä¸å­˜åœ¨total order
```
